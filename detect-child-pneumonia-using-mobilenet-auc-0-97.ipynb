{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7006613,"sourceType":"datasetVersion","datasetId":4027957}],"dockerImageVersionId":29928,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Take Your First Step in Medical Image Classification #\n\n**<font size=\"3\">Artificial intelligence (AI) is getting importance day by day in healthcare. Again, Deep already shows its superhuman accuracy on image classification. So, AI enthusiasts are jumping to develop computer-aided detection systems to diagnose various diseases. Today, We will build a model to detect pediatric/child pneumonia. This kernel is for beginners who want to jump in medical image classification. We will pick a dataset containing pneumonia (bacterial and viral) and normal images. We will do binary (two-class) classification so our model will detect pneumonia from the test set. Data load and preprocessing, Training, and evaluation are basic steps of any classification model. We will follow these steps. There are various ways to preprocess any dataset. We want to do it an easy way so we think beginners can easily understand it. Beginners should follow the comments for better understading. If you don't understand anything or you have more queries, feel free to contact me. Email: sawrupkhan@yahoo.com</font>**","metadata":{}},{"cell_type":"code","source":"#import necessary libraries when needed\nimport os\nimport numpy as np\nimport pandas as pd\nimport pathlib\nimport imageio\n\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T03:36:51.014949Z","iopub.execute_input":"2023-11-20T03:36:51.015286Z","iopub.status.idle":"2023-11-20T03:36:51.020195Z","shell.execute_reply.started":"2023-11-20T03:36:51.015257Z","shell.execute_reply":"2023-11-20T03:36:51.019089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">Now , We jump to get insight of the dataset.\nDataset has three folders (train , val ,test).</font>**\n","metadata":{}},{"cell_type":"code","source":"# Exploring dataset \nbase_dir = '../input/newset/Chest-Xray-Augmented-2/'\ntrain_pneumonia_dir = base_dir+'train/PNEUMONIA/'\ntrain_normal_dir=base_dir+'train/NORMAL/'\n\ntest_pneumonia_dir = base_dir+'test/PNEUMONIA/'\ntest_normal_dir = base_dir+'test/NORMAL/'\n\nval_normal_dir= base_dir+'val/NORMAL/'\nval_pnrumonia_dir= base_dir+'val/PNEUMONIA/'\n\ntrain_pn = [train_pneumonia_dir+\"{}\".format(i) for i in os.listdir(train_pneumonia_dir) ]\ntrain_normal = [train_normal_dir+\"{}\".format(i) for i in os.listdir(train_normal_dir) ]\n\ntest_normal = [test_normal_dir+\"{}\".format(i) for i in os.listdir(test_normal_dir)]\ntest_pn = [test_pneumonia_dir+\"{}\".format(i) for i in os.listdir(test_pneumonia_dir)]\n\nval_pn= [val_pnrumonia_dir+\"{}\".format(i) for i in os.listdir(val_pnrumonia_dir) ]\nval_normal= [val_normal_dir+\"{}\".format(i) for i in os.listdir(val_normal_dir) ]\n\nprint (\"Total images:\",len(train_pn+train_normal+test_normal+test_pn+val_pn+val_normal))\nprint (\"Total pneumonia images:\",len(train_pn+test_pn+val_pn))\nprint (\"Total Nomral images:\",len(train_normal+test_normal+val_normal))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:36:51.025064Z","iopub.execute_input":"2023-11-20T03:36:51.025441Z","iopub.status.idle":"2023-11-20T03:36:51.051058Z","shell.execute_reply.started":"2023-11-20T03:36:51.025394Z","shell.execute_reply":"2023-11-20T03:36:51.050323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preprocessing & Visualization ## \n**<font size=\"3\">The dataset is small. So, We follow 80%, 15%, 5% split. Besides, dataset is clearly imbalanced. We will take care of it later. </font>**","metadata":{}},{"cell_type":"code","source":"# Dataset Splitting (train 80% , test 15% and validation 5% )\n\n# Gathering all pneumina and normal chest X-ray in two python list\npn = train_pn + test_pn + val_pn\nnormal = train_normal + test_normal + val_normal\n\n# Spliting dataset in train set,test set and validation set.\n\ntrain_imgs = pn[:3418]+ normal[:1224]  # 80% of 4273 Pneumonia and normal chest X-ray are 3418 and 1224 respectively.\ntest_imgs = pn[3418:4059]+ normal[1224:1502]\nval_imgs = pn[4059:] + normal[1502:]\n\nprint(\"Total Train Images %s containing %s pneumonia and %s normal images\" \n      % (len(train_imgs),len(pn[:3418]),len(normal[:1224])))\nprint(\"Total Test Images %s containing %s pneumonia and %s normal images\"\n      % (len(test_imgs),len(pn[3418:4059]),len(normal[1224:1502])))\nprint(\"Total validation Images %s containing %s pneumonia and %s normal images\" \n      % (len(val_imgs),len(pn[4059:]),len(normal[1502:])))\n\n\n\nimport random\n\nrandom.shuffle(train_imgs)\nrandom.shuffle(test_imgs)\nrandom.shuffle(val_imgs)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-11-20T03:36:51.052987Z","iopub.execute_input":"2023-11-20T03:36:51.053266Z","iopub.status.idle":"2023-11-20T03:36:51.076620Z","shell.execute_reply.started":"2023-11-20T03:36:51.053238Z","shell.execute_reply":"2023-11-20T03:36:51.075344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing a file name from test set\nprint(test_imgs[5])","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:36:51.078029Z","iopub.execute_input":"2023-11-20T03:36:51.078383Z","iopub.status.idle":"2023-11-20T03:36:51.088561Z","shell.execute_reply.started":"2023-11-20T03:36:51.078347Z","shell.execute_reply":"2023-11-20T03:36:51.087756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">Loading each image and their label into array</font>**","metadata":{}},{"cell_type":"code","source":"import cv2\nimg_size = 224\n\ndef preprocess_image(image_list):\n    \n    X = [] # images\n    y = [] #labels (0 for Normal or 1 for Pneumonia)\n    count=0\n    \n    for image in image_list:\n        \n        try:\n\n            img = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n            \n            img=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_CUBIC)\n            \n            #convert image to 2D to 3D \n            img = np.dstack([img, img, img])\n            \n            #convrt greyscale image to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            # Normalalize Image\n            img = img.astype(np.float32)/255.\n            \n            count=count+1\n\n            X.append(img) \n            \n            \n        except:\n            continue\n        #get the labels \n        if 'NORMAL' in image:\n            y.append(0)\n            \n        elif 'IM' in image:\n            y.append(0)\n            \n        elif 'virus' or 'bacteria' in image:\n            y.append(1)\n            \n    \n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:36:51.089763Z","iopub.execute_input":"2023-11-20T03:36:51.090054Z","iopub.status.idle":"2023-11-20T03:36:51.102080Z","shell.execute_reply.started":"2023-11-20T03:36:51.090011Z","shell.execute_reply":"2023-11-20T03:36:51.100984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX, y = preprocess_image(train_imgs)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:36:51.104777Z","iopub.execute_input":"2023-11-20T03:36:51.105152Z","iopub.status.idle":"2023-11-20T03:37:05.855125Z","shell.execute_reply.started":"2023-11-20T03:36:51.105107Z","shell.execute_reply":"2023-11-20T03:37:05.853977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check all the images getting labels or not\narr=y\t\nuniqueValues, occurCount = np.unique(arr, return_counts=True)\n \nprint(\"Unique Values : \" , uniqueValues)\nprint(\"Occurrence Count : \", occurCount)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:05.857391Z","iopub.execute_input":"2023-11-20T03:37:05.857756Z","iopub.status.idle":"2023-11-20T03:37:05.865442Z","shell.execute_reply.started":"2023-11-20T03:37:05.857721Z","shell.execute_reply":"2023-11-20T03:37:05.864492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some images from train set\n# Feel free to show more image by changing the values\n\nimport matplotlib.pyplot as plt \n\nfig = plt.figure(figsize=(20, 5))\nk=1\nfor i in range(4):\n    a = fig.add_subplot(1, 4, k)\n    if (y[i]==0):\n        a.set_title('Normal')\n    else:\n        a.set_title('Pneumonia')\n        \n    plt.imshow(X[i])\n    k=k+1;\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:05.867565Z","iopub.execute_input":"2023-11-20T03:37:05.867980Z","iopub.status.idle":"2023-11-20T03:37:06.565994Z","shell.execute_reply.started":"2023-11-20T03:37:05.867935Z","shell.execute_reply":"2023-11-20T03:37:06.565023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the labels for test set\n\nP, t = preprocess_image(test_imgs)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:06.567565Z","iopub.execute_input":"2023-11-20T03:37:06.567931Z","iopub.status.idle":"2023-11-20T03:37:09.301523Z","shell.execute_reply.started":"2023-11-20T03:37:06.567891Z","shell.execute_reply":"2023-11-20T03:37:09.300692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=t\t\nuniqueValues, occurCount = np.unique(arr, return_counts=True)\n \nprint(\"Unique Values : \" , uniqueValues)\nprint(\"Occurrence Count : \", occurCount)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:09.302943Z","iopub.execute_input":"2023-11-20T03:37:09.303222Z","iopub.status.idle":"2023-11-20T03:37:09.310201Z","shell.execute_reply.started":"2023-11-20T03:37:09.303194Z","shell.execute_reply":"2023-11-20T03:37:09.309462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now displaying some images from test set\n# Feel free to show more image by changing the values\n\nimport matplotlib.pyplot as plt \n\nfig = plt.figure(figsize=(20, 5))\nk=1\nfor i in range(4):\n    a = fig.add_subplot(1, 4, k)\n    if (t[i]==0):\n        a.set_title('Normal')\n    else:\n        a.set_title('Pneumonia')\n        \n    plt.imshow(P[i])\n    k=k+1;","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:09.311421Z","iopub.execute_input":"2023-11-20T03:37:09.311712Z","iopub.status.idle":"2023-11-20T03:37:09.911388Z","shell.execute_reply.started":"2023-11-20T03:37:09.311679Z","shell.execute_reply":"2023-11-20T03:37:09.910597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the labels for validation set\n\nK, m = preprocess_image(val_imgs)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:09.913188Z","iopub.execute_input":"2023-11-20T03:37:09.913592Z","iopub.status.idle":"2023-11-20T03:37:21.030046Z","shell.execute_reply.started":"2023-11-20T03:37:09.913551Z","shell.execute_reply":"2023-11-20T03:37:21.029310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\narr=m\n\n \n# Get a tuple of unique values & their frequency in numpy array\nuniqueValues, occurCount = np.unique(arr, return_counts=True)\n \nprint(\"Unique Values : \" , uniqueValues)\nprint(\"Occurrence Count : \", occurCount)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:21.033611Z","iopub.execute_input":"2023-11-20T03:37:21.033887Z","iopub.status.idle":"2023-11-20T03:37:21.041133Z","shell.execute_reply.started":"2023-11-20T03:37:21.033859Z","shell.execute_reply":"2023-11-20T03:37:21.040232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now displaying some images from validation set\n# Feel free to show more image by changing the values\n\nimport matplotlib.pyplot as plt \n\nfig = plt.figure(figsize=(20, 5))\nk=1\nfor i in range(4):\n    a = fig.add_subplot(1, 4, k)\n    if (m[i]==0):\n        a.set_title('Normal')\n    else:\n        a.set_title('Pneumonia')\n        \n    plt.imshow(K[i])\n    k=k+1;","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:21.043064Z","iopub.execute_input":"2023-11-20T03:37:21.043465Z","iopub.status.idle":"2023-11-20T03:37:21.699104Z","shell.execute_reply.started":"2023-11-20T03:37:21.043428Z","shell.execute_reply":"2023-11-20T03:37:21.698215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"4\">Let's count and show the quantity of image in each set by counplot from Seaborn </font>**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndf=pd.DataFrame()\ndf['Train']=y\ndf['Test']=pd.Series(t)\ndf['Val']=pd.Series(m)\n\n\n\nfig, ax =plt.subplots(1,3,figsize=(15,7))\nsns.countplot(df['Train'], ax=ax[0])\nax[0].set(ylim=(0, 3500))\n\n\nsns.countplot(df['Test'], ax=ax[1])\nax[1].set(ylim=(0, 3500))\n\nsns.countplot(df['Val'], ax=ax[2])\nax[2].set(ylim=(0, 3500))\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:21.700306Z","iopub.execute_input":"2023-11-20T03:37:21.700611Z","iopub.status.idle":"2023-11-20T03:37:22.025838Z","shell.execute_reply.started":"2023-11-20T03:37:21.700576Z","shell.execute_reply":"2023-11-20T03:37:22.025035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with class imbalance ##\n\n**<font size=\"3\">We can solve the class imbalance problem with several methods. One of the simplest ways to solve the class imbalance is to simply provide a weight for each class which places more emphasis on the minority classes so CNN model can learn equally from all classes. Here, we employ sklearn compute class weight function to set weight for each class. </font>**\n","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y), # here, y contains train set label\n                                                 y)\nclass_weights = dict(enumerate(class_weights))\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:22.026924Z","iopub.execute_input":"2023-11-20T03:37:22.027222Z","iopub.status.idle":"2023-11-20T03:37:22.035741Z","shell.execute_reply.started":"2023-11-20T03:37:22.027191Z","shell.execute_reply":"2023-11-20T03:37:22.034878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport gc\n\ntrain_imgs = train_pn[:3875]+ train_normal[:1341]\ndel train_imgs\ngc.collect()\n\nX_train = np.array(X)\ny_train = np.array(y)\nX_test = np.array(P)\ny_test = np.array(t)\nX_val = np.array(K)\ny_val = np.array(m)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(X_val.shape)\nprint(y_val.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:22.037099Z","iopub.execute_input":"2023-11-20T03:37:22.037399Z","iopub.status.idle":"2023-11-20T03:37:24.002960Z","shell.execute_reply.started":"2023-11-20T03:37:22.037371Z","shell.execute_reply":"2023-11-20T03:37:24.002106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<font size=\"6\"> Training </font>","metadata":{}},{"cell_type":"markdown","source":"\n\n\n**<font size=\"3\">We can say that training phase starts from here. We will use a batch size of 32. Batch size should be a power of 2 (4, 8, 16, 32, 64, 128, 256,....).\nThe batch size 32 means the model will train 32 training samples and then update its parameters once.\nBatch training is faster and memory efficient.</font>**","metadata":{}},{"cell_type":"code","source":"# clear memory\ndel X\ndel y\ngc.collect()\n\n#get the length of the train and validation data\nntrain = len(X_train)\nnval = len(X_val)\n\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:24.004364Z","iopub.execute_input":"2023-11-20T03:37:24.004760Z","iopub.status.idle":"2023-11-20T03:37:24.383439Z","shell.execute_reply.started":"2023-11-20T03:37:24.004717Z","shell.execute_reply":"2023-11-20T03:37:24.382344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Augmentation ##\n**<font size=\"3\">Lets build an input pipline for model with the augmentation technique.\nWe artificially increase the dataset with augmentation technique, since we are using a small dataset. \nIt also helps prevent overfitting. Feel free to study various augmentation techniques in internet. </font>**","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(  rotation_range=7,\n                                     width_shift_range=0.05,\n                                     height_shift_range=0.05,\n                                     shear_range=0.2,\n                                     zoom_range=0.45,\n                                     horizontal_flip=True)\n                                   \nval_datagen = ImageDataGenerator(zoom_range=0.45)  ","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:24.384989Z","iopub.execute_input":"2023-11-20T03:37:24.385316Z","iopub.status.idle":"2023-11-20T03:37:24.392311Z","shell.execute_reply.started":"2023-11-20T03:37:24.385284Z","shell.execute_reply":"2023-11-20T03:37:24.391508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the image generators\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nval_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:24.393994Z","iopub.execute_input":"2023-11-20T03:37:24.394368Z","iopub.status.idle":"2023-11-20T03:37:24.400851Z","shell.execute_reply.started":"2023-11-20T03:37:24.394330Z","shell.execute_reply":"2023-11-20T03:37:24.400106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set image Size\nimg_size =224","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:24.401948Z","iopub.execute_input":"2023-11-20T03:37:24.402243Z","iopub.status.idle":"2023-11-20T03:37:24.409968Z","shell.execute_reply.started":"2023-11-20T03:37:24.402214Z","shell.execute_reply":"2023-11-20T03:37:24.409060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train full train set with MobileNet  ##\n\n**<font size=\"3\"> We don't build a convolutional neural network (CNN) architecture from scratch here. Instead, we use a pretrainted CNN architecture called MobileNet. </font>**","metadata":{}},{"cell_type":"code","source":"from keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.applications import *\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom keras.models import Model\nfrom keras import backend as K\n\n# Create the base pre-trained model\n# Weights should be none becuase we don't need to train with any pre-trained weights here\n\nbase_model = MobileNet(weights=None, include_top=False,input_shape=(img_size, img_size, 3)) \n\nx = base_model.output\n\n# Add a global spatial average pooling layer\nx = GlobalAveragePooling2D()(x)\n\n# Add a logistic layer\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n# Compile model\nmodel.compile(optimizer='adam', loss = 'binary_crossentropy',\n                           metrics = ['binary_accuracy', 'mae'])","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:24.411411Z","iopub.execute_input":"2023-11-20T03:37:24.411821Z","iopub.status.idle":"2023-11-20T03:37:25.708524Z","shell.execute_reply.started":"2023-11-20T03:37:24.411781Z","shell.execute_reply":"2023-11-20T03:37:25.707830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see details of  MobileNet architecure's details\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:25.709497Z","iopub.execute_input":"2023-11-20T03:37:25.709792Z","iopub.status.idle":"2023-11-20T03:37:25.712908Z","shell.execute_reply.started":"2023-11-20T03:37:25.709764Z","shell.execute_reply":"2023-11-20T03:37:25.711954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">Keep your GPU on or training time will be so high. Check your kaggle notebook's settings.**</font>","metadata":{}},{"cell_type":"code","source":"\n# We train for 64 epochs \nhistory = model.fit_generator(train_generator,\n                              steps_per_epoch=200,\n                              epochs=5,\n                              validation_data=val_generator,\n                              validation_steps=nval // batch_size,\n                              class_weight =class_weights,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:37:25.714061Z","iopub.execute_input":"2023-11-20T03:37:25.714314Z","iopub.status.idle":"2023-11-20T03:45:16.864657Z","shell.execute_reply.started":"2023-11-20T03:37:25.714289Z","shell.execute_reply":"2023-11-20T03:45:16.863575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">Plot how validation accuracy and loss are increasing  against training accuracy and loss.</font>**","metadata":{}},{"cell_type":"code","source":"# Lets plot the train and val curve\n# Get the details form the history object\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:45:16.866374Z","iopub.execute_input":"2023-11-20T03:45:16.866795Z","iopub.status.idle":"2023-11-20T03:45:17.333501Z","shell.execute_reply.started":"2023-11-20T03:45:16.866743Z","shell.execute_reply":"2023-11-20T03:45:17.332654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Estimation of classification performance / Result ##\n**<font size=\"3\">We will check our model classification ability using some evaluation matrices. It is very important to use multiple evaluation metrics to evaluate your model to ensure that the model is operating correctly and optimally. We will check our model's performance by Accuracy, Recall, Precision, F1 and AUC score. Feel free to search in google to know more about these matrices. </font>**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = model.predict(X_test)\n\nacc = accuracy_score(y_test, np.round(preds))*100\ncm = confusion_matrix(y_test, np.round(preds))\n\ntn, fp, fn, tp = cm.ravel()\n\nprint('CONFUSION MATRIX ------------------')\nprint(cm)\n\nprint('\\n============TEST METRICS=============')\nprecision = tp/(tp+fp)*100\nrecall = tp/(tp+fn)*100\nprint('Accuracy: {}%'.format(acc))\nprint('Precision: {}%'.format(precision))\nprint('Recall: {}%'.format(recall))\nprint('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n\nprint('\\nTRAIN METRIC ----------------------')\nprint('Train acc: {}'.format(np.round((history.history['binary_accuracy'][-1])*100, 2)))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:45:17.334957Z","iopub.execute_input":"2023-11-20T03:45:17.335278Z","iopub.status.idle":"2023-11-20T03:45:19.876562Z","shell.execute_reply.started":"2023-11-20T03:45:17.335246Z","shell.execute_reply":"2023-11-20T03:45:19.875625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(cm, annot=True, fmt=\"d\",)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:45:19.877726Z","iopub.execute_input":"2023-11-20T03:45:19.878006Z","iopub.status.idle":"2023-11-20T03:45:20.028969Z","shell.execute_reply.started":"2023-11-20T03:45:19.877978Z","shell.execute_reply":"2023-11-20T03:45:20.028244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">The ROC (receiver operating characteristic) curve indicates the diagnostic accuracy and porformance of a model.\nWe show the ROC curve and also calculate AUC score.</font>**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve,roc_auc_score\nfrom sklearn.metrics import auc\n\nfpr , tpr , thresholds = roc_curve ( y_test , preds)\nauc_keras = auc(fpr, tpr)\nprint(\"AUC Score:\",auc_keras)\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % auc_keras)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T03:45:20.030084Z","iopub.execute_input":"2023-11-20T03:45:20.030353Z","iopub.status.idle":"2023-11-20T03:45:20.182804Z","shell.execute_reply.started":"2023-11-20T03:45:20.030327Z","shell.execute_reply":"2023-11-20T03:45:20.181934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<font size=\"3\">We see the model achieves great evaluation scores. It indicates that our model performs very well. But these scores can be increased. Beginners should apply different architectures. Transfer learning can be applied along with different augmentation techniques. If this kernel helps you, you can give an upvote. Stay tuned for more and Happy Coding !! </font>**","metadata":{}}]}